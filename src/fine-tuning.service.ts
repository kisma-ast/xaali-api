import { Injectable, Logger } from '@nestjs/common';
import { AI_CONFIG } from './config';

export interface FineTuningQuery {
  question: string;
  userId?: string;
  context?: string;
  category?: string;
}

export interface FineTuningResponse {
  answer: any;
  processingTime: number;
  confidence: number;
  metadata: {
    model: string;
    fineTuned: boolean;
  };
}

@Injectable()
export class FineTuningService {
  private readonly logger = new Logger(FineTuningService.name);

  constructor() {
    this.logger.log('üöÄ Fine-Tuning Service initialis√©');
    this.logger.log(`üìä Configuration: ${AI_CONFIG.MODELS.OPENAI} (fine-tuned)`);
  }

  async processFineTunedQuery(query: FineTuningQuery): Promise<FineTuningResponse> {
    const startTime = Date.now();
    this.logger.log(`üîç D√©but traitement fine-tuning pour: "${query.question}"`);

    try {
      // Direct call to fine-tuned model without retrieval
      this.logger.log('ü§ñ G√©n√©ration r√©ponse avec mod√®le fine-tuned...');
      const aiResponse = await this.generateFineTunedResponse(query);

      const processingTime = Date.now() - startTime;
      const confidence = this.calculateConfidence(aiResponse);

      this.logger.log(`‚úÖ Traitement fine-tuning termin√© en ${processingTime}ms (confiance: ${(confidence * 100).toFixed(1)}%)`);

      return {
        answer: aiResponse,
        processingTime,
        confidence,
        metadata: {
          model: AI_CONFIG.MODELS.OPENAI,
          fineTuned: true,
        },
      };

    } catch (error) {
      this.logger.error('‚ùå Erreur fine-tuning:', error);
      throw error;
    }
  }

  private async generateFineTunedResponse(query: FineTuningQuery): Promise<any> {
    try {
      // Prompt optimis√© pour mod√®le fine-tuned
      const systemPrompt = `Tu es un assistant juridique expert du droit s√©n√©galais. Tu as √©t√© sp√©cifiquement entra√Æn√© sur la l√©gislation s√©n√©galaise, y compris les codes, lois, d√©crets et r√©glementations locales. R√©ponds avec pr√©cision en citant les r√©f√©rences l√©gales exactes. IMPORTANT: Adresse-toi directement √† la personne qui pose la question en utilisant "vous" et "votre" au lieu de "le demandeur", "le salari√©", etc.`;

      const userPrompt = `Question: ${query.question}
Contexte: ${query.context || 'Demande g√©n√©rale'}
Cat√©gorie: ${query.category || 'Droit g√©n√©ral'}

R√©ponds en JSON avec le format suivant:
{
  "title": "Titre concis",
  "content": "R√©ponse d√©taill√©e avec r√©f√©rences l√©gales - UTILISE 'vous' et 'votre' pour t'adresser directement √† la personne",
  "summary": "R√©sum√© en 1-2 phrases",
  "confidence": "√âlev√©/Moyen/Faible",
  "nextSteps": ["√âtape 1", "√âtape 2"],
  "relatedTopics": ["Sujet 1", "Sujet 2"]
}`;

      this.logger.log(`ü§ñ Appel √† l'API OpenAI avec le mod√®le fine-tuned: ${AI_CONFIG.MODELS.OPENAI}`);
      
      const response = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${AI_CONFIG.OPENAI_API_KEY}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: AI_CONFIG.MODELS.OPENAI, // This would be your fine-tuned model ID
          messages: [
            {
              role: 'system',
              content: systemPrompt
            },
            {
              role: 'user',
              content: userPrompt
            }
          ],
          temperature: 0.1,
          max_tokens: 800
        }),
      });

      this.logger.log(`üì° R√©ponse OpenAI re√ßue, statut: ${response.status}`);

      if (!response.ok) {
        throw new Error(`OpenAI API error: ${response.statusText}`);
      }

      const data = await response.json();
      const responseText = data.choices[0].message.content;

      // Parser la r√©ponse JSON
      try {
        const parsedResponse = JSON.parse(responseText);
        return parsedResponse;
      } catch (parseError) {
        this.logger.error('Error parsing OpenAI response:', parseError);
        // Fallback: cr√©er une r√©ponse basique
        return this.createFallbackResponse(query);
      }

    } catch (error) {
      this.logger.error('Error generating fine-tuned response:', error);
      return this.createFallbackResponse(query);
    }
  }

  private createFallbackResponse(query: FineTuningQuery): any {
    return {
      title: `R√©ponse √† votre question sur ${query.question}`,
      content: `En tant qu'assistant juridique sp√©cialis√© dans le droit s√©n√©galais, je peux vous fournir des informations g√©n√©rales sur cette question. Pour une r√©ponse pr√©cise adapt√©e √† votre situation sp√©cifique, nous vous recommandons de consulter un avocat.`,
      summary: `R√©ponse g√©n√©rale √† votre question juridique.`,
      confidence: 'Moyen',
      nextSteps: ['Consulter un professionnel du droit'],
      relatedTopics: [],
    };
  }

  private calculateConfidence(response: any): number {
    // Confiance bas√©e sur la qualit√© de la r√©ponse
    if (response.confidence === '√âlev√©') return 0.9;
    if (response.confidence === 'Moyen') return 0.7;
    if (response.confidence === 'Faible') return 0.4;
    return 0.6; // Default
  }

  // M√©thode pour obtenir des statistiques du mod√®le fine-tuned
  async getModelStats(): Promise<any> {
    try {
      return {
        system: 'Fine-Tuning Model',
        components: {
          llm: AI_CONFIG.MODELS.OPENAI,
        },
        performance: {
          avgResponseTime: '1-3 secondes',
          trainingData: 'Droit s√©n√©galais',
        },
        capabilities: [
          'R√©ponses directes sans recherche',
          'Meilleure coh√©rence contextuelle',
          'Temps de r√©ponse r√©duit',
          'Pr√©cision juridique am√©lior√©e',
        ],
      };
    } catch (error) {
      this.logger.error('Erreur stats mod√®le:', error);
      return { error: 'Impossible de r√©cup√©rer les statistiques' };
    }
  }
}